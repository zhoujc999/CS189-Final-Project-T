{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras_mnist.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDMKdFHIKASs"
      },
      "source": [
        "# Convolutional Neural Network Training using Keras on Google Cloud Platform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TJSVZIaLwYu"
      },
      "source": [
        "# Overview\n",
        "\n",
        "Google Cloud Platform (GCP) is a suite of cloud computing services that runs on the same infrastructure that Google uses internally for its end-user products. Alongside a set of management tools, it provides infrastructure as a service, platform as a service, and serverless computing environments for various applications such as computing, data storage, data analytics and machine learning. In this assigment, we are going to set up a Jupyter Notebook on Google Cloud Platform and use Keras to build and train a Convolutional Neural Network (CNN) on the MNIST dataset.\n",
        "\n",
        "MNIST is a widely used dataset for handwritten digit classification. It consists of 70,000 labeled $28 \\times 28$ pixel grayscale images of hand-written digits. The dataset is split into 60,000 training images and 10,000 test images. There are 10 classes (one for each of the 10 digits). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWA9qzpSL6lQ"
      },
      "source": [
        "## Setup\n",
        "\n",
        "1. First, we need to create a free account on Google Cloud Platform and create a new project as shown in the screenshot below.\n",
        "\n",
        "> ![New Project in GCP](https://raw.githubusercontent.com/zhoujc999/CS189-Final-Project-T/master/Assets/Screenshot_4.png)\n",
        "\n",
        "---\n",
        "2. We need to create a VM instance to run our Jupyter notebook on the cloud. To do this, click on the compute option and select ‘VM Instances’.\n",
        "\n",
        "> ![New Project in GCP](https://raw.githubusercontent.com/zhoujc999/CS189-Final-Project-T/master/Assets/Screenshot_5.png)\n",
        "\n",
        "---\n",
        "3. After setting up the VM instance according to your preference, we are ready to tweak the firewall settings to enable us to access our Jupyter notebook on our local browser. Note: Select the 2 options as shown below to allow SSH connections.\n",
        "\n",
        "> ![New Project in GCP](https://raw.githubusercontent.com/zhoujc999/CS189-Final-Project-T/master/Assets/Screenshot_6.png)\n",
        "\n",
        "---\n",
        "4. Under 'VPC network', click on 'Firewall' to add firewall rules to our newly created VM instance. Under 'Source IP Ranges', input `0.0.0.0/0` to allow connections from all IPs. Under 'Protocol and Ports', input your desired port for your browser to connect to the Jupyter notebook on Google Cloud Platform. Click on create after you are satisfied with the details.\n",
        "\n",
        "> ![New Project in GCP](https://raw.githubusercontent.com/zhoujc999/CS189-Final-Project-T/master/Assets/Screenshot_7.png)\n",
        "\n",
        "---\n",
        "5. We need to return to the VM instances interface to start our VM instance. Take note of the external IP address.\n",
        "\n",
        "> ![New Project in GCP](https://raw.githubusercontent.com/zhoujc999/CS189-Final-Project-T/master/Assets/Screenshot_8.png)\n",
        "\n",
        "---\n",
        "6. After connecting to the VM instance via SSH, we need to install Jupyter notebook. In the console, type the following.\n",
        "```shell\n",
        "> apt install jupyter\n",
        "```\n",
        "\n",
        "---\n",
        "7. Once the relevant packages are installed, we want to install PyTorch, which contains the building blocks for building our convolutional neural network.\n",
        "```shell\n",
        "> pip3 install keras\n",
        "```\n",
        "\n",
        "---\n",
        "8. Next, we need to configure our Jupyter notebook to accept connections from our browser. After we generate the configuration file, we add the following lines of code in `~/.jupyter/jupyter_notebook_config.py`. Note: `<PORT_NUMBER>` is the port you've chosen for the firewall settings.\n",
        "```shell\n",
        "> jupyter notebook --generate-config\n",
        "```\n",
        "```python\n",
        "c = get_config()\n",
        "c.NotebookApp.ip = '*'\n",
        "c.NotebookApp.open_browser = False\n",
        "c.NotebookApp.port = <PORT_NUMBER>\n",
        "```\n",
        "\n",
        "---\n",
        "9. Finally, we are able to launch the Jupyter notebook with the following command. Connect to the notebook from our browser by going to the following address: `http://<EXTERNAL_IP_ADDRESS>:<PORT_NUMBER>`\n",
        "```shell\n",
        "jupyter-notebook --no-browser --port=<PORT_NUMBER>\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcg0x18Q-lb4"
      },
      "source": [
        "Now, we have successfully set-up our Jupyter notebook on Google Cloud Platform and we are ready to build and train our CNN classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ad4wQvlBf4W-"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INP4ikaSOu-O"
      },
      "source": [
        "### Getting the *data*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTBOQWex8yUd"
      },
      "source": [
        "First, we need to download the MNIST dataset with the following code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0_qnANutbAy"
      },
      "source": [
        "```python\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data(path=\"mnist.npz\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u72jVMIb-NiZ"
      },
      "source": [
        "Loading the dataset returns four NumPy arrays:\n",
        "\n",
        "- The train_images and train_labels arrays are the training set—the data the model uses to learn.\n",
        "- The model is tested against the test set, the test_images, and test_labels arrays.\n",
        "\n",
        "The images are $28 \\times 28$ NumPy arrays, with pixel values ranging from 0 to 255. The labels are an array of integers, ranging from 0 to 9, corresponding the digits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQDl6PvbOrUQ"
      },
      "source": [
        "### Preprocess the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qgs-dlqUaob"
      },
      "source": [
        "Before we feed the image data into the neural network, we need to preprocess the data to enable the network to learn better. One such procedure is to scale these values to a range of 0 to 1 before feeding them to the neural network model. To do so, divide the values by 255. It's important that the training set and the testing set be preprocessed in the same way."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifaFFwfnWT57"
      },
      "source": [
        "```python\n",
        "train_images = train_images / 255.0\n",
        "\n",
        "test_images = test_images / 255.0\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0IUcI_4Wly0"
      },
      "source": [
        "## Convolution Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wO0cbmcsIJK"
      },
      "source": [
        "### Build the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-MQfVgGbPSX"
      },
      "source": [
        "You will be writing some code to build the neural network and configure the layers of the model. The basic building block of a neural network is the layer. Layers extract representations from the data fed into them. Hopefully, these representations are meaningful for the problem at hand.\n",
        "\n",
        "Similar to the assignment using PyTorch on AWS, we suggest the following architecture:\n",
        "\n",
        "1. A convolutional layer with 10 output channels and kernel size of 5\n",
        "2. A max pool layer with kernel of size 2 and stride of 2\n",
        "3. A ReLU activation layer\n",
        "4. A convolutional layer with 20 output channels and kernel size of 5\n",
        "5. A dropout layer with probability 0.5\n",
        "6. A max pool layer with stride equals 2\n",
        "7. A ReLU activation layer\n",
        "8. 2 fully connected layers with a dropout layer in between\n",
        "9. A softmax output layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keh_H9QZb1nI"
      },
      "source": [
        "```python\n",
        "from tensorflow.keras import Sequential, layers\n",
        "\n",
        "model = Sequential([\n",
        "    layers.Flatten(input_shape=(28, 28)),\n",
        "    layers.Conv2D(10, 5),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Activation(activations.relu),\n",
        "    layers.Conv2D(20, 5),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Activation(activations.relu),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(50),\n",
        "    tf.keras.layers.Dense(10),\n",
        "    layers.Softmax()\n",
        "])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ft00iBYksO6_"
      },
      "source": [
        "### Compile the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oi95rVAzpQzA"
      },
      "source": [
        "Before the model is ready for training, it needs a few more settings. These are added during the model's compile step:\n",
        "\n",
        "- Loss function: This measures how accurate the model is during training. You want to minimize this function to \"steer\" the model in the right direction. We will use the sparse categorical cross-entropy loss as it is the most suitable for our problem.\n",
        "- Optimizer: This is how the model is updated based on the data it sees and its loss function. In this case, we will use the Adam optimizer, which has proved to be one of best-performing optimizer under most conditions.\n",
        "- Metrics: Used to monitor the training and testing steps. We use accuracy, the fraction of the images that are correctly classified."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtjVEivhpeqj"
      },
      "source": [
        "```python\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtSNev0bsEbi"
      },
      "source": [
        "### Train the model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbM6ai2ZsV1o"
      },
      "source": [
        "Training the neural network model requires the following steps:\n",
        "\n",
        "1. Feed the training data to the model. In this example, the training data is in the train_images and train_labels arrays.\n",
        "2. The model learns to associate images and labels.\n",
        "\n",
        "To start training, call the `model.fit` method — so called because it \"fits\" the model to the training data:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j46IsCQWsnYe"
      },
      "source": [
        "```python\n",
        "model.fit(train_images, train_labels, epochs=10)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Q411oQ6tSUV"
      },
      "source": [
        "### Evaluate the accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-t1-D5nWtWGt"
      },
      "source": [
        "Next, compare how the model performs on the test dataset:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBwss-E0t511"
      },
      "source": [
        "```python\n",
        "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Vzyr8XQsxou"
      },
      "source": [
        "Report the training and test accuracy:\n",
        "\n",
        "Training accuracy = _________\n",
        "\n",
        "Test accuracy = _________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_1LZX3mb1RN"
      },
      "source": [
        "### Make predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gD-jXznDvBp1"
      },
      "source": [
        "With the model trained, you can use it to make predictions about some images. For example, we can make a prediction of the first image in `test_images`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-QcjtJyvq-u"
      },
      "source": [
        "```python\n",
        "predictions = probability_model.predict(test_images)\n",
        "np.argmax(predictions[0])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZSDsoUEw7T0"
      },
      "source": [
        "Report the prediction of the first image:\n",
        "\n",
        "Prediction = _________\n"
      ]
    }
  ]
}