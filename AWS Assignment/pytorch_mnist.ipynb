{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Environment (conda_pytorch_p27)",
      "language": "python",
      "name": "conda_pytorch_p27"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.14"
    },
    "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.",
    "colab": {
      "name": "pytorch_mnist.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "sBc1U-rgUzro"
      },
      "source": [
        "# Convolutional Neural Network Training using PyTorch in AWS SageMaker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmirDqUgUzrp"
      },
      "source": [
        "# Overview\n",
        "\n",
        "Amazon Web Services (AWS) offers a wide range of tools and functionalities for enterprise and individual developers. Among which, SageMaker is a fully managed machine learning service that allows data scientists and developers to build and train machine learning models, and S3, is a data storage service providing the capability to store the dataset that powers our model training process. In this assigment, we are going to use PyTorch in AWS SageMaker to implement and train a Convolutional Neural Network (CNN) on the MNIST dataset.\n",
        "\n",
        "MNIST is a widely used dataset for handwritten digit classification. It consists of 70,000 labeled 28x28 pixel grayscale images of hand-written digits. The dataset is split into 60,000 training images and 10,000 test images. There are 10 classes (one for each of the 10 digits). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfQSRp0pUzrp"
      },
      "source": [
        "## Setup\n",
        "\n",
        "1. Let's start by creating a SageMaker session as shown in the screenshot below. If any of these steps are confusing, we reccomend watching this video we created: https://youtu.be/s1M8P9X6j_8 or this tutorial on Amazon SageMaker: https://www.youtube.com/watch?v=pfjhNe1M2t4 \n",
        "\n",
        "> ![SageMaker Studio Session](https://raw.githubusercontent.com/zhoujc999/CS189-Final-Project-T/master/Assets/Screenshot_1.png)\n",
        "\n",
        "---\n",
        "\n",
        "2. Configure the IAM role to access the S3 bucket. This is necessary to store and retrieve the dataset use for training and evaluation. The S3 bucket should be within the same region as the Notebook Instance, training, and hosting.\n",
        "\n",
        "> ![IAM role](https://raw.githubusercontent.com/zhoujc999/CS189-Final-Project-T/master/Assets/Screenshot_2.png)\n",
        "\n",
        "---\n",
        "\n",
        "3. Finally, create the Jupyter Notebook instance.\n",
        "\n",
        "> ![SageMaker Studio Session](https://raw.githubusercontent.com/zhoujc999/CS189-Final-Project-T/master/Assets/Screenshot_3.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBGg-ANXUzrp"
      },
      "source": [
        "## Data\n",
        "\n",
        "### Getting the data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sagemaker\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "\n",
        "sagemaker_session = sagemaker.Session()\n",
        "bucket = sagemaker_session.default_bucket()\n",
        "role = sagemaker.get_execution_role()\n",
        "\n",
        "!pip install --upgrade jupyter_client\n",
        "\n",
        "datasets.MNIST('data', download=True, transform=transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YYmKKxdUzrp"
      },
      "source": [
        "### Uploading the data to S3\n",
        "We are going to use the `sagemaker.Session.upload_data` function to upload our datasets to an S3 location. The return value inputs identifies the location -- we will use later when we start the training job.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BDGMEW0Uzrp"
      },
      "source": [
        "bucket = sagemaker_session.default_bucket()\n",
        "prefix = 'sagemaker/pytorch-mnist'\n",
        "inputs = sagemaker_session.upload_data(path='data', bucket=bucket, key_prefix=prefix)\n",
        "print('S3 path: {}'.format(inputs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmqdP8iMUzrp"
      },
      "source": [
        "\n",
        "## Convolutional Neural Network\n",
        "\n",
        "You will be writing some code to complete the implementation of the CNN model to recognise handwritten digits. In the provided `mnist.py` script, there are a few parts that needs to be filled before we can start training the model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAV0aQjrUzrp"
      },
      "source": [
        "### Part (a) & (b)\n",
        "\n",
        "First, we need to define the CNN model. Since the input data are $28 \\times 28$ pixel grayscale images, the input to the CNN has a single channel. We suggest the following architecture:\n",
        "\n",
        "1. A convolutional layer with 10 output channels and kernel size of 5\n",
        "2. A max pool layer with kernel of size 2 and stride of 2\n",
        "3. A ReLU activation layer\n",
        "4. A convolutional layer with 20 output channels and kernel size of 5\n",
        "5. A dropout layer with probability 0.5\n",
        "6. A max pool layer with stride equals 2\n",
        "7. A ReLU activation layer\n",
        "8. 2 fully connected layers with a dropout layer in between\n",
        "9. A softmax output layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nD8DKCVzUzrp"
      },
      "source": [
        "### Part (c)\n",
        "\n",
        "Next, we focus on the `train()` function. After setting up the function, we define an optimizer to start training the model. In part (c), we use batch Stochastic Gradient Descent to optimize our CNN model. Here are the steps we need to accomplish:\n",
        "\n",
        "1. For each batch, reset the optimizer gradients to 0.\n",
        "2. Feed the data into the model and generate the output.\n",
        "3. Compute the loss of the model\n",
        "4. Back propagate the weights of the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MwxgVLnUzrp"
      },
      "source": [
        "### Part (d)\n",
        "\n",
        "Finally, we can implement the `test()` function in part (d). To complete the function:\n",
        "\n",
        "1. Feed the data into the model and generate the output.\n",
        "2. Compute the negative log likelihood loss.\n",
        "3. Increment the correct counter if the prediction was correct.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfkyiubyUzrp"
      },
      "source": [
        "## Run training in SageMaker\n",
        "\n",
        "Now, we are ready to train our model on SageMaker. The `PyTorch` class allows us to run our training function as a training job on SageMaker infrastructure. We need to configure it with our training script, an IAM role, the number of training instances, the training instance type, and hyperparameters. This example can be ran on one or multiple, cpu or gpu instances. The hyperparameters parameter is a dict of values that will be passed to your training script -- you can see how to access these values in the `mnist.py` script above.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMWPSMrgUzrp"
      },
      "source": [
        "from sagemaker.pytorch import PyTorch\n",
        "\n",
        "estimator = PyTorch(entry_point='mnist.py',\n",
        "                    role=role,\n",
        "                    framework_version='1.4.0',\n",
        "                    train_instance_count=2,\n",
        "                    train_instance_type='ml.c4.xlarge',\n",
        "                    hyperparameters={\n",
        "                        'epochs': 6,\n",
        "                        'backend': 'gloo'\n",
        "                    })"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TB_bI-H7Uzrp"
      },
      "source": [
        "After we've constructed our `PyTorch` object, we can fit it using the data we uploaded to S3. SageMaker makes sure our data is available in the local filesystem, so our training script can simply read the data from disk.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBRyFO1tUzrp"
      },
      "source": [
        "estimator.fit({'training': inputs})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxKfq3w1Uzrp"
      },
      "source": [
        "## Host\n",
        "### Create endpoint\n",
        "After training, we use the `PyTorch` estimator object to build and deploy a `PyTorchPredictor`. This creates a Sagemaker Endpoint -- a hosted prediction service that we can use to perform inference.\n",
        "\n",
        "The arguments to the deploy function allow us to set the number and type of instances that will be used for the Endpoint. These do not need to be the same as the values we used for the training job. For example, you can train a model on a set of GPU-based instances, and then deploy the Endpoint to a fleet of CPU-based instances, but you need to make sure that you return or save your model as a cpu model similar to what we did in `mnist.py`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictor = estimator.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
      ]
    }
  ]
}
